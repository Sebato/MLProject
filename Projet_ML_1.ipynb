{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sebato/MLProject/blob/main/Projet_ML_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f3764e",
      "metadata": {
        "id": "77f3764e"
      },
      "outputs": [],
      "source": [
        "# Importation des différentes librairies utiles pour le notebook\n",
        "\n",
        "!pip install langdetect \n",
        "!pip install contractions\n",
        "!pip install wordcloud\n",
        "\n",
        "#Sickit learn met régulièrement à jour des versions et \n",
        "#indique des futurs warnings. \n",
        "#ces deux lignes permettent de ne pas les afficher.\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# librairies générales\n",
        "import pickle \n",
        "import pandas as pd\n",
        "from scipy.stats import randint\n",
        "import numpy as np\n",
        "import string\n",
        "import time\n",
        "import base64\n",
        "import re\n",
        "import sys\n",
        "\n",
        "import contractions\n",
        "\n",
        "# librairie BeautifulSoup\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# librairie affichage\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import wordcloud\n",
        "\n",
        "## detection de language\n",
        "import langdetect \n",
        "\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk import RegexpParser\n",
        "# il est possible de charger l'ensemble des librairies en une seule fois \n",
        "# décocher le commentaire de la ligne ci-dessous\n",
        "#nltk.download('all') \n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "nltk.download('tagsets')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "from spacy.tokens import Span\n",
        "# il faut sélectionner pour quelle langue les traitements vont être faits.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Récupération des données\n"
      ],
      "metadata": {
        "id": "8AC0o7q4Q7_W"
      },
      "id": "8AC0o7q4Q7_W"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "my_local_drive='/content/drive/MyDrive/ProjetML'\n",
        "# Ajout du path pour les librairies, fonctions et données\n",
        "sys.path.append(my_local_drive)\n",
        "# Se positionner sur le répertoire associé\n",
        "%cd $my_local_drive\n",
        "\n",
        "%pwd"
      ],
      "metadata": {
        "id": "iGN2MhSNh3Ii"
      },
      "id": "iGN2MhSNh3Ii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65efafa4",
      "metadata": {
        "id": "65efafa4"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ee39e5f",
      "metadata": {
        "id": "4ee39e5f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df_train = pd.read_csv('data/HAI817_Projet_train.csv')\n",
        "# df.drop_duplicates(subset=['title'], keep='first', inplace = True)\n",
        "display(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"A\": [\"TeamA\", \"TeamB\", \"TeamB\", \"TeamC\", \"TeamA\"],\n",
        "    \"B\": [50, 40, 40, 30, 50],\n",
        "    \"C\": [True, False, False, False, True]\n",
        "}\n",
        "  \n",
        "df = pd.DataFrame(data)\n",
        "display(df)\n",
        "display(df.drop_duplicates())"
      ],
      "metadata": {
        "id": "gzWxd3FK2uTS"
      },
      "id": "gzWxd3FK2uTS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display(dd)"
      ],
      "metadata": {
        "id": "OJjBCFuUVe4Y"
      },
      "id": "OJjBCFuUVe4Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a068e3ef",
      "metadata": {
        "id": "a068e3ef"
      },
      "source": [
        "# Préparation des données\n",
        "- Tokenization et mise en lowercase\n",
        "### Création de liste des mots qui apparaissent dans le texte et dans le titre de chaque document "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaningData(data):\n",
        " \"\"\"etapes de nettoyage:\n",
        "  - supprimer doublons\n",
        "  - suppression de la fin des URLs\n",
        "  - transformation des caractères numériques en mots\n",
        "  - \n",
        "      \n",
        "  \"\"\"\n",
        "  # regex = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
        "  # tokens = re.findall(regex, text.lower()) # notice lowercase=True param\n",
        "  # vocab = sorted(set(tokens))\n",
        "  # counts = Counter(tokens)\n",
        "  # counts = [counts[key] for key in sorted(counts.keys())]\n",
        "  # #vocab, counts = list(zip(*sorted(counter.items()))) # one liner with asterisk unpacking\n",
        "  # print(pd.DataFrame([counts], columns = vocab)) \n"
      ],
      "metadata": {
        "id": "4KvD1Copt_rs"
      },
      "id": "4KvD1Copt_rs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "#Prise en compte des éléments qui doivent être regroupés\n",
        "regex_str = [\n",
        "    r'<[^>]+>', # HTML tags\n",
        "    r'(?:@[\\w_]+)', # @-mentions\n",
        "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "    # r'(^($|£)[\\d]+(,[\\d]+)+(.[\\d]{2})?)' # Monnaies\n",
        "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        " \n",
        "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # nombres\n",
        "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # mots avec - et '\n",
        "    r'(?:[\\w_]+){2,}' # autres mots\n",
        "    #r'(?:\\S)' # le reste\n",
        "]\n",
        "\n",
        "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "\n",
        "# def tokenize(s):\n",
        "#   tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "  \n",
        "#   return tokens_re.findall(s)\n",
        "\n",
        "def tokenize(s):\n",
        "    ps = nltk.stem.porter.PorterStemmer()\n",
        "    stop_words = list(set(stopwords.words(\"english\")))\n",
        "    tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "    tokens = tokens_re.findall(s)\n",
        "    stems = [ps.stem(t) for t in tokens if t not in stop_words]\n",
        "    return stems\n",
        "\n",
        "    \n",
        "tokenPattern = r'('+'|'.join(regex_str)+')'\n",
        "print(\"\\n--------\\n\"+tokenPattern)\n",
        "    \n",
        "# def preprocess(text):\n",
        "#   \"\"\" etapes de pre processing :\n",
        "#       - suppréssion de la fin des URLs\n",
        "#       - transformation des caractères numériques en mots\n",
        "#       - \n",
        "      \n",
        "#   \"\"\"\n",
        "#   ps = nltk.stem.porter.PorterStemmer()\n",
        "#   text = \" \".join([ps.stem(w) for w in text.split()])\n",
        "#   return text.lower()\n",
        "  "
      ],
      "metadata": {
        "id": "qFO8n28SmGze"
      },
      "id": "qFO8n28SmGze",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6bd6dae",
      "metadata": {
        "id": "f6bd6dae"
      },
      "outputs": [],
      "source": [
        "def get_tokens(dataframe):    \n",
        "    full_texts = []\n",
        "    for i in range(len(dataframe)):\n",
        "        full_texts.append((str(dataframe.iloc[i].title) + \" \" + str(dataframe.iloc[i].text)))\n",
        "    tokens = [t.split() for t in full_texts]\n",
        "    tokens = [[t.lower() for t in lt] for lt in tokens]\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76d05256",
      "metadata": {
        "id": "76d05256"
      },
      "outputs": [],
      "source": [
        "def elim_stopwords(words, stopwords):\n",
        "    return [word for word in words if word not in stopwords]\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722bc21b",
      "metadata": {
        "id": "722bc21b"
      },
      "outputs": [],
      "source": [
        "def stemming(dataframe):\n",
        "    \"\"\" \"\"\"\n",
        "    ps=nltk.stem.porter.PorterStemmer()\n",
        "    dataframe[\"tokens\"].transform(lambda tokens: [ps.stem(word) for word in tokens])\n",
        "    return dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6947d5",
      "metadata": {
        "id": "bd6947d5"
      },
      "outputs": [],
      "source": [
        "def prepare_data(dataframe, classes):\n",
        "    dataframe[\"tokens\"] = get_tokens(dataframe)\n",
        "    \n",
        "    swords = set(stopwords.words(\"english\"))\n",
        "    dataframe[\"tokens\"] = dataframe[\"tokens\"].transform(lambda t: [word for word in t if word not in swords])\n",
        "    \n",
        "    #retirer émoticones\n",
        "    \n",
        "\n",
        "    ps=nltk.stem.porter.PorterStemmer()\n",
        "    dataframe[\"tokens\"] = dataframe[\"tokens\"].transform(lambda tokens: [ps.stem(word) for word in tokens])\n",
        "\n",
        "    dataframe = dataframe[dataframe[\"our rating\"].isin(classes)]\n",
        "    dataframe = dataframe.reset_index(drop=True)\n",
        "        \n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e7726c",
      "metadata": {
        "id": "f1e7726c"
      },
      "outputs": [],
      "source": [
        "#df_train = prepare_data(df_train, [\"true\", \"false\"])\n",
        "#(display(df_train.head()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a374a65",
      "metadata": {
        "id": "1a374a65"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def create_bag(dataframe):\n",
        "    corpus = []\n",
        "    for i in range(len(dataframe)):\n",
        "        corpus.append((str(dataframe.iloc[i].title) + \" \" + str(dataframe.iloc[i].text)))\n",
        "    vectorizer = CountVectorizer(tokenizer=tokenize)\n",
        "    # creation du vocabulaire\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "    df = pd.DataFrame(data=vectorizer.transform(corpus).toarray(), columns=vectorizer.get_feature_names_out())\n",
        "    df.insert(0, \"our_rating\", dataframe[\"our rating\"])\n",
        "    return (vectorizer, df)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f0cd7a3",
      "metadata": {
        "id": "9f0cd7a3"
      },
      "outputs": [],
      "source": [
        "bag, bag_train = create_bag(df_train)\n",
        "print(bag.vocabulary_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\"\"\"etapes de nettoyage:\n",
        "  - Supression des lignes avec deux rating different\n",
        "  - Supression des doublons\n",
        "  - Supression des lignes qui n'ont pas texte et titre\n",
        "\"\"\"\n",
        "def cleaningData(df):\n",
        "  df = pd.read_csv('data/HAI817_Projet_train.csv')\n",
        "  df_merge = pd.merge(df, df, how='inner', left_on=['title', 'text'], right_on=['title', 'text'])\n",
        "  df_supr = df_merge[(df_merge['our rating_x'] != df_merge['our rating_y'])]\n",
        "  df_supr = df[df['public_id'].isin(df_supr['public_id_x'].drop_duplicates().values)]\n",
        "  df.drop(df_supr.index, inplace = True)\n",
        "  df.drop_duplicates(subset=['title'], keep=\"first\", inplace = True)\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "OqdBz53MtwZP"
      },
      "id": "OqdBz53MtwZP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersect(bag_train, bag_test, df_train, df_test):\n",
        "  vocab_train = list(bag_train.vocabulary_.keys())\n",
        "  vocab_test = list(bag_test.vocabulary_.keys())\n",
        "\n",
        "  inter = [mot for mot in vocab_train if mot in vocab_test]\n",
        "\n",
        "  train_drop = [mot for mot in vocab_train if mot not in inter]\n",
        "  test_drop =  [mot for mot in vocab_test  if mot not in inter]\n",
        "\n",
        "  df_train = df_train.drop(train_drop, 1)\n",
        "  df_test = df_test.drop(test_drop, 1)\n",
        "\n",
        "  return (df_train, df_test)"
      ],
      "metadata": {
        "id": "L4yzwWMXz9Cy"
      },
      "id": "L4yzwWMXz9Cy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cellule définition:\n",
        "- Tokenizer\n",
        "- Bag of words\n",
        "- drop duplicates\n",
        "###renvoie les données et leur classe pour application directe d'un modèle"
      ],
      "metadata": {
        "id": "z7WTgArtlVmV"
      },
      "id": "z7WTgArtlVmV"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "#trouve les mots en communs entre les deux datasets et supprime les autres\n",
        "def intersect(bag_train, bag_test, df_train, df_test):\n",
        "  vocab_train = list(bag_train.vocabulary_.keys())\n",
        "  vocab_test = list(bag_test.vocabulary_.keys())\n",
        "\n",
        "  inter = [mot for mot in vocab_train if mot in vocab_test]\n",
        "\n",
        "  # print(inter[:20])\n",
        "  \n",
        "  # train_drop = [mot for mot in vocab_train if mot not in inter]\n",
        "  # test_drop = [mot for mot in vocab_test if mot not in inter]\n",
        "\n",
        "  train_drop = []\n",
        "  test_drop = []\n",
        "\n",
        "  for mot in vocab_train:\n",
        "    if mot not in inter:\n",
        "      train_drop.append(mot)\n",
        " \n",
        "  for mot in vocab_test:\n",
        "    if mot not in inter:\n",
        "      test_drop.append(mot)\n",
        "  \n",
        "  df_train = df_train.drop(train_drop, 1)\n",
        "  df_test = df_test.drop(test_drop, 1)\n",
        "\n",
        "  # display(df_train.head())\n",
        "  # display(df_test.head())\n",
        "\n",
        "  return (df_train, df_test)\n",
        "\n",
        "\n",
        "#Mise en forme des données (suppression doublons, sans titres)\n",
        "def cleaningData(df):\n",
        "  df.rename(columns={ df.columns[0]: \"public_id\" }, inplace = True)\n",
        "  df_merge = pd.merge(df, df, how='inner', left_on=['title', 'text'], right_on=['title', 'text'])\n",
        "  df_supr = df_merge[(df_merge['our rating_x'] != df_merge['our rating_y'])]\n",
        "  df_supr = df[df['public_id'].isin(df_supr['public_id_x'].drop_duplicates().values)]\n",
        "  df.drop(df_supr.index, inplace = True)\n",
        "  df.drop_duplicates(subset=['title'], keep=\"first\", inplace = True)\n",
        "  df.reset_index(inplace = True, drop = True)\n",
        "  return df\n",
        "\n",
        "\n",
        "#Prise en compte des éléments qui doivent être regroupés\n",
        "regex_str = [\n",
        "    r'<[^>]+>', # HTML tags\n",
        "    r'(?:@[\\w_]+)', # @-mentions\n",
        "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
        "    # r'(^($|£)[\\d]+(,[\\d]+)+(.[\\d]{2})?)' # Monnaies\n",
        "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
        "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # nombres\n",
        "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # mots avec - et '\n",
        "    r'(?:[\\w_]+){2,}' # autres mots\n",
        "    #r'(?:\\S)' # le reste\n",
        "]\n",
        "\n",
        "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "\n",
        "#tokenizer amélioré (stopwords; regex; stems; )\n",
        "def tokenize(s):\n",
        "    ps = nltk.stem.porter.PorterStemmer()\n",
        "    stop_words = list(set(stopwords.words(\"english\")))\n",
        "    tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
        "    tokens = tokens_re.findall(s)\n",
        "    stems = [ps.stem(t) for t in tokens if t not in stop_words]\n",
        "    return stems\n",
        "\n",
        "#bag of words\n",
        "def create_bag(dataframe):\n",
        "    corpus = []\n",
        "    for i in range(len(dataframe)):\n",
        "        corpus.append((str(dataframe.iloc[i].title) + \" \" + str(dataframe.iloc[i].text)))\n",
        "    vectorizer = CountVectorizer(max_df=0.5, tokenizer=tokenize)\n",
        "    # creation du vocabulaire\n",
        "    vector = vectorizer.fit_transform(corpus)\n",
        "    df = pd.DataFrame(data=vectorizer.transform(corpus).toarray(), columns=vectorizer.get_feature_names_out())\n",
        "    df.insert(0, \"our_rating\", dataframe[\"our rating\"])\n",
        "    return (vectorizer, df)\n",
        "    \n",
        "#appel de tout\n",
        "def process(data):\n",
        "  \n",
        "  # 1 : suppression doublons \n",
        "  data = cleaningData(data)\n",
        "\n",
        "  # 2 : bag of words\n",
        "  bag, df = create_bag(data)\n",
        "\n",
        "  #3 : return bag and data\n",
        "  return(bag, df)"
      ],
      "metadata": {
        "id": "UkiPRK6Ef4VZ"
      },
      "id": "UkiPRK6Ef4VZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cellule execution:\n",
        "\n"
      ],
      "metadata": {
        "id": "hZwZdzL1mGlO"
      },
      "id": "hZwZdzL1mGlO"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "df1 = pd.read_csv('data/HAI817_Projet_train.csv')\n",
        "df2 = pd.read_csv('data/HAI817_Projet_test.csv')\n",
        "\n",
        "bag_train, df_train = process(df1)\n",
        "\n",
        "bag_test, df_test = process(df2)\n",
        "\n",
        "# print(len(bag_test.vocabulary_))\n",
        "# print(len(bag_train.vocabulary_))\n",
        "\n",
        "df_1, df_2 = intersect(bag_train, bag_test, df_train, df_test)\n",
        "\n",
        "# df_1 = df_train\n",
        "# df_2 = df_test"
      ],
      "metadata": {
        "id": "b2gWi72VmGP1"
      },
      "id": "b2gWi72VmGP1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_bin = df_1[(df_1['our_rating'] == 'true') | (df_1['our_rating'] == 'false')]\n",
        "df_test_bin = df_2[(df_2['our_rating'] == 'true') | (df_2['our_rating'] == 'false')]\n",
        "\n",
        "\n",
        "# print(df_train_bin['our_rating'].values_count())\n",
        "\n",
        "X_train = df_train_bin.values[:, 1:]\n",
        "y_train = df_train_bin.values[:, 0]\n",
        "\n",
        "X_test = df_test_bin.values[:, 1:]\n",
        "y_test = df_test_bin.values[:, 0]\n",
        "\n"
      ],
      "metadata": {
        "id": "IBpY-mMki_1b"
      },
      "id": "IBpY-mMki_1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# choix du classifieur\n",
        "clf = LogisticRegression(max_iter=1500)\n",
        "\n",
        "# entrainement\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# application\n",
        "res = clf.predict(X_test)\n",
        "\n",
        "print(f'accuracy = {accuracy_score(res, y_test)}')\n"
      ],
      "metadata": {
        "id": "0_5ZOn4Lt_L9"
      },
      "id": "0_5ZOn4Lt_L9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48087bd3",
      "metadata": {
        "id": "48087bd3"
      },
      "outputs": [],
      "source": [
        "bag_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(bag_train)"
      ],
      "metadata": {
        "id": "7-DPAeZzrVm2"
      },
      "id": "7-DPAeZzrVm2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bag_train.to_csv(\"bow.csv\")"
      ],
      "metadata": {
        "id": "tpVrraha6Id-"
      },
      "id": "tpVrraha6Id-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GifxDtFM6pyw"
      },
      "id": "GifxDtFM6pyw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf7a2e6",
      "metadata": {
        "id": "3cf7a2e6"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = bag_train.values[:, 1:]\n",
        "y = bag_train.values[:, 0]\n"
      ],
      "metadata": {
        "id": "ZSHkxZ8ClH2-"
      },
      "id": "ZSHkxZ8ClH2-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression(max_iter=3000)))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVC', SVC(gamma='auto')))\n",
        "models.append(('LSVC', LinearSVC()))\n",
        "models.append(('RFC', RandomForestClassifier()))\n",
        "\n",
        "seed = 7\n",
        "results = []\n",
        "names = []\n",
        "scoring='accuracy'\n",
        "for name,model in models:\n",
        "    kfold = KFold(n_splits=10, random_state=seed,shuffle=True)\n",
        "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg) \n"
      ],
      "metadata": {
        "id": "3m35J_urk-CZ"
      },
      "id": "3m35J_urk-CZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab6ff22",
      "metadata": {
        "id": "7ab6ff22"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(max_iter=3000)\n",
        "clf.fit(X, y)\n",
        "res = clf.score(X, y)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeef88e6",
      "metadata": {
        "id": "eeef88e6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}